{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc81985-985f-43aa-b8c0-b51c9a933c80",
   "metadata": {},
   "source": [
    "# Extractive Summarization\n",
    "1. Using the spaCy library\n",
    "2. Using GloVe embeddings and cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7993e776-0ac5-4e85-ab85-96d27c2ce607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 20:29:41.368367: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-03 20:29:41.406073: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-03 20:29:41.406621: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-03 20:29:42.069957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-12-03 20:29:45.875088: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99152cc2-e075-4e6f-a490-4e37d1508ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import process_text, extract_text_from_url\n",
    "from spacy_helper import summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f46581-0f12-4688-b183-105db97adaae",
   "metadata": {},
   "source": [
    "### Get and process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f89b95c-f7cc-487e-86f3-af327664a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
    "text = extract_text_from_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f87d1fd4-36c0-4995-ab52-710c89f2c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1246bd9b-c241-4f32-98bb-ac9a375ee738",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences = [process_text(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4fbb78a-0e62-49c3-a654-a8ba22462e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7140fbd4-0de1-44fd-9fc1-c728b9887485",
   "metadata": {},
   "source": [
    "## Using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291cf5fa-b567-4108-9635-70fe4ac1bbf2",
   "metadata": {},
   "source": [
    "### spaCy's approach\n",
    "- Tokenize the sentence \n",
    "- Produce a word count and normalize over total words\n",
    "- Calculate the sum of the normalized count for each sentence.\n",
    "- Percentage of these sentences form the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802acd15-9c0f-4773-8d11-c17cac689672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the helper function contains the details of matrix building,\n",
    "# the notebook is for presentation\n",
    "# the second argument (0.1) returns the top 10% most similar sentences \n",
    "sum_text = summarize(text, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98215366-2d0e-4c38-a386-c65a00eed61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_sent = nltk.sent_tokenize(sum_text)\n",
    "len(st_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96834fb5-c798-4684-b70d-550da9219bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That popularity was due partly to a flurry of results showing that such techniques[10][11] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[12] and parsing.',\n",
       " '[13][14] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[15] or protect patient privacy.',\n",
       " '[16]\\n Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[17][18] such as by writing grammars or devising heuristic rules for stemming.In 2003, word n-gram model, at the time the best statistical algorithm, was overperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.',\n",
       " '[8]\\n In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[9] and in the following years he went on to develop Word2vec.As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[47] with two defining aspects:\\n Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s.Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881cdbe-00ef-42fd-a5c8-9a3f9b2b6128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19bd85fa-4432-4864-a32d-76bf769b0c4a",
   "metadata": {},
   "source": [
    "## Using word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d2ab0-5cc7-4e42-9055-54496f505811",
   "metadata": {},
   "source": [
    "### Using the Stanford GloVe embeddings: https://github.com/stanfordnlp/GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78cc2e9d-658a-4f6e-93fb-6790bd77c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-03 20:29:47--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-12-03 20:29:47--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-12-03 20:29:48--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  2.79MB/s    in 4m 48s  \n",
      "\n",
      "2023-12-03 20:34:37 (2.85 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7681585b-3bdc-44e4-aee0-b1fa3d45935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors; using (100,) sized vectors \n",
    "word_embeddings = {}\n",
    "\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    word_embeddings[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c18e95c-0de9-4b58-9532-376d59be61d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "\n",
    "for sent in clean_sentences:\n",
    "    sent_words = sent.split()\n",
    "    if len(sent) != 0:\n",
    "        # sum the word embeddings for each word of the sentence\n",
    "        # normalize across the sentence with laplacian smoothing\n",
    "        vec = sum([word_embeddings.get(word, np.zeros((100, )))\n",
    "                   for word in sent_words])/(len(sent_words) + 0.001)\n",
    "    else:\n",
    "        vec = np.zeros((100, ))\n",
    "    sentence_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e5b19c5-6f17-413a-a05a-2ab3463c4024",
   "metadata": {},
   "outputs": [],
   "source": [
    "simularity_matrix = np.zeros([len(clean_sentences), len(clean_sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d182406-0e1a-4749-95d7-1aecc2c1ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(clean_sentences)):\n",
    "  for j in range(len(clean_sentences)):\n",
    "    if i != j:\n",
    "      simularity_matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9ec38e7-1a08-41f9-9bcd-46a455d9743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a graph enables identification of the key nodes (sentences) in the document\n",
    "nx_graph = nx.from_numpy_array(simularity_matrix)\n",
    "\n",
    "# pagerank algorithm scores sentences by number of connections/similarity \n",
    "scores = nx.pagerank(nx_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f914f56a-6e89-4d91-a40a-4d2086cbda63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(clean_sentences)), reverse=True)\n",
    "len(ranked_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43683905-4328-49a4-9022-54da46d1eb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as example george lakoff offers methodology build natural language processing nlp algorithms perspective cognitive science along findings cognitive linguistics 47 two defining aspects ties cognitive linguistics part historical heritage nlp less frequently addressed since statistical turn 1990s\n",
      "that popularity due partly flurry results showing techniques 10 11 achieve state-of-the-art results many natural language tasks e.g. language modeling 12 parsing\n",
      "machine learning approaches include statistical neural networks hand many advantages symbolic approach although rule-based systems manipulating symbols still use 2020 become mostly obsolete advance llms 2023\n",
      "in 2010s representation learning deep neural network-style featuring many hidden layers machine learning methods became widespread natural language processing\n",
      "53 likewise ideas cognitive nlp inherent neural models multimodal nlp although rarely made explicit 54 developments artificial intelligence specifically tools technologies using large language model approaches\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    print(ranked_sentences[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8141a88-042c-4216-86ae-765b552b55c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d82d54-7f11-430e-9800-646260b105c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca2af28-4aa9-42a1-9cb7-d826acaa2421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
